# ================================================================================
import os,shutil
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

import torch
import torch.nn as nn
from torch.utils.data import Dataset
from torch.optim import Adam

from datasets import load_dataset # pip install datasets

import transformers
from transformers import pipeline
from transformers import GPT2LMHeadModel
from transformers import PreTrainedTokenizerFast
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline
from transformers import Trainer, TrainingArguments, AutoModelWithLMHead
from transformers.models.gpt2.configuration_gpt2 import GPT2Config
from transformers.models.gpt2.modeling_gpt2 import GPT2Model
from transformers import AutoTokenizer, BloomTokenizerFast
from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer

from chatgpt.experience_maker import NaiveExperienceMaker
from chatgpt.experience_maker import NaiveExperienceMaker
from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy
from chatgpt.dataset import RewardDataset
from chatgpt.models.bloom import BLOOMRM
from chatgpt.models.gpt import GPTRM
from chatgpt.models.opt import OPTRM
from chatgpt.trainer import RewardModelTrainer
# from ..base import RewardModel
from chatgpt.models.base import RewardModel, Actor
from chatgpt.models.bloom import BLOOMActor, BLOOMCritic
from chatgpt.models.gpt import GPTActor, GPTCritic
from chatgpt.models.opt import OPTActor, OPTCritic
from chatgpt.trainer import PPOTrainer

import loralib as lora
from colossalai.nn.optimizer import HybridAdam

from copy import deepcopy
import pandas as pd
import argparse
import copy
import logging
import json
from dataclasses import dataclass, field
from typing import Optional, Dict, Sequence
import random

# ================================================================================
random.seed(230319)

# ================================================================================
# Load pretrained GPT-2 model (Text completion model)

def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    """Collects the state dict and dump to disk."""
    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa

# ================================================================================
# define argment
parser = argparse.ArgumentParser()
parser.add_argument('--data_path_1_SFT', type=str, default='./data_kochatgpt/kochatgpt_1_SFT.jsonl')
parser.add_argument('--model_name', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])
parser.add_argument('--max_epochs', type=int, default=2)
parser.add_argument('--train_batch_size', type=int, default=2)
parser.add_argument('--output_dir', type=str, default='./output_1_SFT')

args = parser.parse_args(args=[])

# for test
args.model_name = 'skt/kogpt2-base-v2'  # SK GPT2, https://github.com/SKT-AI/KoGPT2
# args.model_name = 'ajoublue-gpt2-base'  # ì•„ì£¼ëŒ€, https://github.com/HeegyuKim/language-model

args.max_epochs = 2

print('args:\n',args)

# ================================================================================
# Pretrained tokenizer

tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',
                                                    pad_token='<pad>', mask_token='<mask>')
print('Tokenize the sentence by pretrained model (skt/kogpt2-base-v2): ',tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o"))
# ['â–ì•ˆë…•', 'í•˜', 'ì„¸', 'ìš”.', 'â–í•œêµ­ì–´', 'â–G', 'P', 'T', '-2', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ˜¤', ':)', 'l^o']

# ================================================================================
# Pretrained GPT-2 model

model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')
# print('model',model)
# GPT2LMHeadModel(
#   (transformer): GPT2Model(
#     (wte): Embedding(51200, 768)
#     (wpe): Embedding(1024, 768)
#     (drop): Dropout(p=0.1, inplace=False)
#     (h): ModuleList(
#     ...
#         (mlp): GPT2MLP(
#           (c_fc): Conv1D()
#           (c_proj): Conv1D()
#           (act): NewGELUActivation()
#           (dropout): Dropout(p=0.1, inplace=False)
#         )
#       )
#     )
#     (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
#   )
#   (lm_head): Linear(in_features=768, out_features=51200, bias=False)
# )

# Incompleted input text to GPT-2
text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'

# Tokenize text
input_ids = tokenizer.encode(text, return_tensors='pt')
# print('input_ids',input_ids)
# input_ids tensor([[33245, 10114, 12748, 11357]])

# Generated text from GPT-2
# Question text token IDs + Answer text token IDs
gen_ids = model.generate(input_ids,
                         max_length=128,
                         repetition_penalty=2.0,
                         pad_token_id=tokenizer.pad_token_id,
                         eos_token_id=tokenizer.eos_token_id,
                         bos_token_id=tokenizer.bos_token_id,
                         use_cache=True)
# print('gen_ids',gen_ids)
# print('gen_ids',gen_ids.shape)
# gen_ids tensor([[33245, 10114, 12748, 11357, 23879, 39306,  9684,  7884, 10211, 15177,
#          26421,   387, 17339,  7889,  9908, 15768,  6903, 15386,  8146, 12923,
#           9228, 18651, 42600,  9564, 17764,  9033,  9199, 14441,  7335,  8704,
#          12557, 32030,  9510, 18595,  9025, 10571, 25741, 10599, 13229,  9508,
#           7965,  8425, 33102,  9122, 21240,  9801, 32106, 13579, 12442, 13235,
#          19430,  8022, 12972,  9566, 11178,  9554, 24873,  7198,  9391, 12486,
#           8711,  9346,  7071, 36736,  9693, 12006,  9038, 10279, 36122,  9960,
#           8405, 10826, 18988, 25998,  9292,  7671,  9465,  7489,  9277, 10137,
#           9677,  9248,  9912, 12834, 11488, 13417,  7407,  8428,  8137,  9430,
#          14222, 11356, 10061,  9885, 19265,  9377, 20305,  7991,  9178,  9648,
#           9133, 10021, 10138, 30315, 21833,  9362,  9301,  9685, 11584,  9447,
#          42129, 10124,  7532, 17932, 47123, 37544,  9355, 15632,  9124, 10536,
#          13530, 12204,  9184, 36152,  9673,  9788,  9029, 11764]])
# gen_ids torch.Size([1, 128])

generated = tokenizer.decode(gen_ids[0])
# print(generated)
# ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ë¬´ì—‡ë³´ë‹¤ ê·œì¹™ì ì¸ ìƒí™œìŠµê´€ì´ ì¤‘ìš”í•˜ë‹¤.
# íŠ¹íˆ, ì•„ì¹¨ì‹ì‚¬ëŠ” ë‹¨ë°±ì§ˆê³¼ ë¹„íƒ€ë¯¼ì´ í’ë¶€í•œ ê³¼ì¼ê³¼ ì±„ì†Œë¥¼ ë§ì´ ì„­ì·¨í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.
# ë˜í•œ í•˜ë£¨ 30ë¶„ ì´ìƒ ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ëŠ” ê²ƒë„ ë„ì›€ì´ ëœë‹¤.
# ì•„ì¹¨ ì‹ì‚¬ë¥¼ ê±°ë¥´ì§€ ì•Šê³  ê·œì¹™ì ìœ¼ë¡œ ìš´ë™ì„ í•˜ë©´ í˜ˆì•¡ìˆœí™˜ì— ë„ì›€ì„ ì¤„ ë¿ë§Œ ì•„ë‹ˆë¼ ì‹ ì§„ëŒ€ì‚¬ë¥¼ ì´‰ì§„í•´ ì²´ë‚´ ë…¸íë¬¼ì„ ë°°ì¶œí•˜ê³  í˜ˆì••ì„ ë‚®ì¶°ì¤€ë‹¤.
# ìš´ë™ì€ í•˜ë£¨ì— 10ë¶„ ì •ë„ë§Œ í•˜ëŠ” ê²Œ ì¢‹ìœ¼ë©° ìš´ë™ í›„ì—ëŠ” ë°˜ë“œì‹œ ìŠ¤íŠ¸ë ˆì¹­ì„ í†µí•´ ê·¼ìœ¡ëŸ‰ì„ ëŠ˜ë¦¬ê³  ìœ ì—°ì„±ì„ ë†’ì—¬ì•¼ í•œë‹¤.
# ìš´ë™ í›„ ë°”ë¡œ ì ìë¦¬ì— ë“œëŠ” ê²ƒì€ í”¼í•´ì•¼ í•˜ë©° íŠ¹íˆ ì•„ì¹¨ì— ì¼ì–´ë‚˜ë©´ ëª¸ì´ í”¼ê³¤í•´ì§€ê¸° ë•Œë¬¸ì— ë¬´ë¦¬í•˜ê²Œ ì›€ì§ì´ë©´ ì˜¤íˆë ¤ ì—­íš¨ê³¼ê°€ ë‚  ìˆ˜ë„ ìˆë‹¤.
# ìš´ë™ì„

# ================================================================================
# How to use pretrained GPT-2 by pipeline

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
generation_args = dict(
    num_beams=4,
    repetition_penalty=2.0,
    no_repeat_ngram_size=4,
    eos_token_id=375, # \n
    max_new_tokens=64,
    do_sample=True,
    top_k=50,
    early_stopping=True
)

# 0:, \n, 1: are needed
aa=generator(
    ["0 : **ëŠ” ê²Œì„ ì¢‹ì•„í•˜ë‹ˆ\n1 :",
     "0 : ì–´ì œ ê°•ë‚¨ì—ì„œ ì‚´ì¸ì‚¬ê±´ ë‚¬ëŒ€ ã…œã…œ ë„ˆë¬´ ë¬´ì„œì›Œ\n1 : í— ì™œ? ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?\n0 : ì‚¬ì§„ë³´ë‹ˆê¹Œ ë§‰ í”¼í˜ë¦¬ëŠ” ì‚¬ëŒìˆê³  ê²½ì°°ë“¤ì´ ë– ì„œ ì œì••í•˜ê³  ë‚œë¦¬ë„ ì•„ë‹ˆì—ˆë‹¤ë˜ë°??\n1 :",
     "0 : ìê¸°ì•¼ ì–´ì œëŠ” ë‚˜í•œí…Œ ì™œ ê·¸ë¬ì–´?\n1 : ë­” ì¼ ìˆì—ˆì–´?\n0 : ì–´ë–»ê²Œ ë‚˜í•œí…Œ ë§ë„ ì—†ì´ ê·¸ëŸ´ ìˆ˜ ìˆì–´? ë‚˜ ì§„ì§œ ì‹¤ë§í–ˆì–´\n1 : "], **generation_args)

# ================================================================================
for one in aa:
    print(one[0]['generated_text'])
    print()

# 0 : **ëŠ” ê²Œì„ ì¢‹ì•„í•˜ë‹ˆ
# 1 : ***ëŠ” ê²Œì„ ëª»í•˜ëŠ”ë°


# 0 : ì–´ì œ ê°•ë‚¨ì—ì„œ ì‚´ì¸ì‚¬ê±´ ë‚¬ëŒ€ ã…œã…œ ë„ˆë¬´ ë¬´ì„œì›Œ
# 1 : í— ì™œ? ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?
# 0 : ì‚¬ì§„ë³´ë‹ˆê¹Œ ë§‰ í”¼í˜ë¦¬ëŠ” ì‚¬ëŒìˆê³  ê²½ì°°ë“¤ì´ ë– ì„œ ì œì••í•˜ê³  ë‚œë¦¬ë„ ì•„ë‹ˆì—ˆë‹¤ë˜ë°??
# 1 : ì•„ê¹Œë„ ë§í–ˆì§€ë§Œ ì´ë²ˆì— ë˜ ì‚´ì¸ì„ ì €ì§ˆëŸ¬ì„œ ì§„ì§œ ì£„ì†¡í•©ë‹ˆë‹¤ á„á„á„


# 0 : ìê¸°ì•¼ ì–´ì œëŠ” ë‚˜í•œí…Œ ì™œ ê·¸ë¬ì–´?
# 1 : ë­” ì¼ ìˆì—ˆì–´?
# 0 : ì–´ë–»ê²Œ ë‚˜í•œí…Œ ë§ë„ ì—†ì´ ê·¸ëŸ´ ìˆ˜ ìˆì–´? ë‚˜ ì§„ì§œ ì‹¤ë§í–ˆì–´
# 1 : ë­˜ í–ˆì–´?
# 2 : ë­˜ í•˜ê³  ì‹¶ì—ˆì–´?
# 3 : ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆì–´?
# 4 : ë­ë¼ê³  í–ˆì–´?
# 5 : ë­ë¼ê³  ë§í–ˆì–´?
# 6 : ë­ë¼ê³  ë§í•´?
# 7 : ë­ë¼ê³  ë§í•˜ì§€ ì•Šì•˜ì–´?
# 8 : ë­ë¼ê³  í•˜ì§€ ì•Šì•˜ì–´?
# 9 : ë­ë¼ê³  ë§í•œ ê±°ì•¼